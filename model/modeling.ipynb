{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import kss\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet & Data Loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self, data, label):\n",
    "    super().__init__()\n",
    "    self.docs = data\n",
    "    self.label = label\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.docs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    news = self.docs[index]\n",
    "    target = self.label[index]\n",
    "    return news, target\n",
    "\n",
    "train_data = MyDataset(data=X_train, label=y_train)\n",
    "val_data = MyDataset(data=X_val, label=y_val)\n",
    "test_data = MyDataset(data=X_test, label=y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self, file_path):\n",
    "    super().__init__()\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-Medium\", do_lower_case=False)\n",
    "    self.bert = AutoModel.from_pretrained(\"snunlp/KR-Medium\")\n",
    "    for param in self.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    df = pd.read_json(file_path)\n",
    "    for idx in range(len(df)) :\n",
    "      tokenized = self.tokenizer(df.iloc[idx,0], padding='longest', return_tensors='pt')\n",
    "      contextualized_sentences = self.bert(**tokenized)\n",
    "      sentence_embeddings = contextualized_sentences.pooler_output\n",
    "      df.iat[idx,1] = sentence_embeddings\n",
    "    self.vecs = df['vecs']\n",
    "    self.label = df['label']\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.vecs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    docs = self.vecs[index]\n",
    "    label = self.label[index]\n",
    "    return docs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('data/전세사기_라벨링.json')\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# sentence_embeddings = j[0]\n",
    "# max_len = max([e.size(0) for e in sentence_embeddings])\n",
    "# padded_embeddings = torch.zeros(len(sentence_embeddings), max_len, sentence_embeddings[0].size(1))\n",
    "# for i, emb in enumerate(sentence_embeddings):\n",
    "#     seq_len = emb.size(0)\n",
    "#     padded_embeddings[i, :seq_len, :] = emb\n",
    "    \n",
    "# random_tensor = torch.randn(padded_embeddings.size(0), 1, padded_embeddings.size(2))\n",
    "# batch_tensor = torch.cat((random_tensor, padded_embeddings), dim=1)\n",
    "# batch_t = batch_tensor.permute(1 ,0 ,2).float()\n",
    "\n",
    "# padding_mask = batch_t.sum(dim=-1).permute(1 ,0) == 0\n",
    "# title_level = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=768, nhead=1),num_layers=1)\n",
    "# output_batch = title_level(batch_t.float(), src_key_padding_mask=padding_mask)\n",
    "# output_batch = output_batch.permute(1 ,0 ,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for문 Ver.\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, num_classes, input_dim, num_heads, num_layers):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(input_dim, num_classes)\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-Medium\", do_lower_case=False)\n",
    "    self.bert = AutoModel.from_pretrained(\"snunlp/KR-Medium\")\n",
    "    for param in self.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    self.title_level = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads),\n",
    "      num_layers=num_layers)\n",
    "    self.sentecne_level = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads),\n",
    "      num_layers=num_layers)\n",
    "\n",
    "  def forward(self, data):\n",
    "    doc_ems = torch.empty((len(data), 768)).to(device)\n",
    "    for num, doc in enumerate(data):\n",
    "      tokenized = self.tokenizer(doc, padding='longest', return_tensors='pt').to(device)\n",
    "      with torch.no_grad():\n",
    "        contextualized_sentences = self.bert(**tokenized)\n",
    "      sentence_embeddings = contextualized_sentences.pooler_output\n",
    "      title = sentence_embeddings[0, :]\n",
    "      transformer_output = self.title_level(sentence_embeddings)[1:, :]\n",
    "      combined_input = torch.cat((title.unsqueeze(1).view(1, 768), transformer_output), dim=0)\n",
    "      doc_ems[num] = self.sentecne_level(combined_input)[0]\n",
    "    out = self.fc(doc_ems)\n",
    "    predicted_label = torch.argmax(out).to(device)\n",
    "    return out, predicted_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Batch Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc_Encoder(nn.Module):\n",
    "    def __init__(self, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.sentecne_level = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=768, nhead=num_heads),num_layers=num_layers)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        max_len = max([e.size(0) for e in batch])\n",
    "        padded_embeddings = torch.zeros(len(batch), max_len, batch[0].size(1)).to(device)\n",
    "        for i, emb in enumerate(batch):\n",
    "            seq_len = emb.size(0)\n",
    "            padded_embeddings[i, :seq_len, :] = emb\n",
    "        random_tensor = torch.randn(padded_embeddings.size(0), 1, padded_embeddings.size(2)).to(device)\n",
    "        batch_tensor = torch.cat((random_tensor, padded_embeddings), dim=1)\n",
    "        batch_tensor = batch_tensor.permute(1 ,0 ,2).float()\n",
    "        padding_mask = batch_tensor.sum(dim=-1).permute(1 ,0) == 0\n",
    "        output_batch = self.sentecne_level(batch_tensor.float(), src_key_padding_mask=padding_mask)\n",
    "        output_batch = output_batch.permute(1 ,0 ,2)\n",
    "        doc_vecs = output_batch[:,0,:]\n",
    "        return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Doc_Encoder(nn.Module):\n",
    "#     def __init__(self, num_classes, num_heads, num_layers):\n",
    "#         super().__init__()\n",
    "#         self.fc = nn.Linear(768, num_classes)\n",
    "#         self.title_level = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=768, nhead=num_heads),num_layers=num_layers)\n",
    "#         self.sentecne_level = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=768, nhead=num_heads),num_layers=num_layers)\n",
    "\n",
    "#     def forward(self, batch):\n",
    "#         max_len = max([e.size(0) for e in batch])\n",
    "#         padded_embeddings = torch.zeros(len(batch), max_len, batch[0].size(1)).to(device)\n",
    "#         for i, emb in enumerate(batch):\n",
    "#             seq_len = emb.size(0)\n",
    "#             padded_embeddings[i, :seq_len, :] = emb\n",
    "#         title = padded_embeddings[:,0,:].reshape(padded_embeddings.size(0), 1, padded_embeddings.size(2))\n",
    "#         padded_embeddings = padded_embeddings.permute(1 ,0 ,2).float()\n",
    "#         padding_mask = padded_embeddings.sum(dim=-1).permute(1 ,0) == 0\n",
    "#         title_level_out = self.title_level(padded_embeddings.float(), src_key_padding_mask=padding_mask)\n",
    "#         title_level_out = title_level_out.permute(1 ,0 ,2)\n",
    "        \n",
    "#         batch_tensor = torch.cat((title, title_level_out), dim=1)\n",
    "#         random_tensor = torch.randn(batch_tensor.size(0), 1, batch_tensor.size(2)).to(device)\n",
    "#         batch_tensor = torch.cat((random_tensor, batch_tensor), dim=1)\n",
    "#         batch_tensor = batch_tensor.permute(1 ,0 ,2).float()\n",
    "#         padding_mask = batch_tensor.sum(dim=-1).permute(1 ,0) == 0\n",
    "#         output_batch = self.sentecne_level(batch_tensor.float(), src_key_padding_mask=padding_mask)\n",
    "#         output_batch = output_batch.permute(1 ,0 ,2)\n",
    "#         doc_vecs = output_batch[:,0,:]\n",
    "#         return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module) :\n",
    "    def __init__(self, num_classes, encoder) :\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(768,num_classes)\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def forward(self, batch) :\n",
    "        doc_vecs = self.encoder(batch)\n",
    "        out = self.fc1(doc_vecs)\n",
    "        out = self.ac1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter & Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_classes = 2\n",
    "input_dim = 768\n",
    "num_heads = 1\n",
    "num_layers = 1\n",
    "encoder = Doc_Encoder(num_heads, num_layers)\n",
    "model = Model(num_classes, encoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training - 문제의식 data로 encoder 추가 학습할 수 있는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 16.749486923217773\n",
      "Epoch 2, Loss: 17.352962493896484\n",
      "Epoch 3, Loss: 16.8115291595459\n",
      "Epoch 4, Loss: 16.901926040649414\n",
      "Epoch 5, Loss: 17.09539794921875\n",
      "Epoch 6, Loss: 17.021961212158203\n",
      "Epoch 7, Loss: 16.325613021850586\n",
      "Epoch 8, Loss: 16.613815307617188\n",
      "Epoch 9, Loss: 17.53157615661621\n",
      "Epoch 10, Loss: 17.18819236755371\n",
      "Epoch 11, Loss: 17.07434844970703\n",
      "Epoch 12, Loss: 16.776493072509766\n",
      "Epoch 13, Loss: 16.350135803222656\n",
      "Epoch 14, Loss: 16.65325927734375\n",
      "Epoch 15, Loss: 17.337514877319336\n",
      "Epoch 16, Loss: 17.100589752197266\n",
      "Epoch 17, Loss: 16.71356201171875\n",
      "Epoch 18, Loss: 16.622323989868164\n",
      "Epoch 19, Loss: 16.54018783569336\n",
      "Epoch 20, Loss: 16.5841007232666\n",
      "Epoch 21, Loss: 16.895811080932617\n",
      "Epoch 22, Loss: 16.49324607849121\n",
      "Epoch 23, Loss: 17.039358139038086\n",
      "Epoch 24, Loss: 16.944934844970703\n",
      "Epoch 25, Loss: 16.87317657470703\n",
      "Epoch 26, Loss: 16.473180770874023\n",
      "Epoch 27, Loss: 16.401714324951172\n",
      "Epoch 28, Loss: 16.719398498535156\n",
      "Epoch 29, Loss: 17.756261825561523\n",
      "Epoch 30, Loss: 16.293476104736328\n",
      "Epoch 31, Loss: 17.195354461669922\n",
      "Epoch 32, Loss: 16.861940383911133\n",
      "Epoch 33, Loss: 16.43246078491211\n",
      "Epoch 34, Loss: 16.08525276184082\n",
      "Epoch 35, Loss: 17.1414737701416\n",
      "Epoch 36, Loss: 16.721288681030273\n",
      "Epoch 37, Loss: 15.773422241210938\n",
      "Epoch 38, Loss: 15.949854850769043\n",
      "Epoch 39, Loss: 16.104713439941406\n",
      "Epoch 40, Loss: 16.142078399658203\n",
      "Epoch 41, Loss: 17.227325439453125\n",
      "Epoch 42, Loss: 16.85788917541504\n",
      "Epoch 43, Loss: 15.482857704162598\n",
      "Epoch 44, Loss: 16.219282150268555\n",
      "Epoch 45, Loss: 16.375730514526367\n",
      "Epoch 46, Loss: 16.720949172973633\n",
      "Epoch 47, Loss: 16.660648345947266\n",
      "Epoch 48, Loss: 17.09074592590332\n",
      "Epoch 49, Loss: 16.434667587280273\n",
      "Epoch 50, Loss: 16.93769073486328\n"
     ]
    }
   ],
   "source": [
    "model.train()  # 모델을 훈련 모드로 설정\n",
    "model.to(device)\n",
    "for epoch in range(50) :\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = tuple(d.to(device) for d in data)\n",
    "        target = torch.tensor(target).long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) \n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = model.to(device)\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, target in train_loader:\n",
    "        target = torch.tensor(target).long()\n",
    "        target = target.to(device)\n",
    "\n",
    "        out = model(batch_data)\n",
    "        predicted = torch.argmax(out, dim=1)\n",
    "        total_samples += target.size(0)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8096590909090909\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few-shot learning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class few_shot_Model(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc1 = nn.Linear(768,768)\n",
    "        self.ac = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(768,256)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, doc_pair):\n",
    "        doc_pair = self.encoder(doc_pair)\n",
    "        doc_pair = self.fc1(doc_pair)\n",
    "        doc_pair = self.ac(doc_pair)\n",
    "        doc_pair = self.fc2(doc_pair)\n",
    "        similarity = torch.cosine_similarity(doc_pair[0], doc_pair[1], dim=0)\n",
    "        out = self.sigmoid(similarity)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>내용</th>\n",
       "      <th>sliced</th>\n",
       "      <th>embedding</th>\n",
       "      <th>cluster</th>\n",
       "      <th>세부분류</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>오거돈 \"3년간 청년정책 사업에 4,900억 투입\"</td>\n",
       "      <td>민선 7기 2년차 첫 정책 '청년정책로드맵' 발표 2022년까지 106개 청년사업 ...</td>\n",
       "      <td>[민선 7기 2년차 첫 정책 '청년정책로드맵' 발표 2022년까지 106개 청년사업...</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>안양청년 89명 청년정책 주도…희망발전 가동</td>\n",
       "      <td>최대호 안양시장 10일 청년정책 서포터즈 위촉장 전달. 사진제공=안양시    【파이...</td>\n",
       "      <td>[최대호 안양시장 10일 청년정책 서포터즈 위촉장 전달., 사진제공=안양시    【...</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>서울시, '2020 청년정책 협력포럼' 개최</td>\n",
       "      <td>서울시가 '청년기본법 이후, 청년의 자리'를 주제로 '2020 청년정책 협력포럼'을...</td>\n",
       "      <td>[서울시가 '청년기본법 이후, 청년의 자리'를 주제로 '2020 청년정책 협력포럼'...</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>양평군, 2020~2024년 청년정책 윤곽 드러나</td>\n",
       "      <td>‘청년실태조사·정책기본계획’ 수립용역 보고회 【양평=뉴시스】 문영일 기자 = 경기 ...</td>\n",
       "      <td>[‘청년실태조사·정책기본계획’ 수립용역 보고회 【양평=뉴시스】 문영일 기자 = 경기...</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>대통령 표창으로 꽃 피운 대구시 청년정책…“청년이 돌아오는 대구 만들 것”</td>\n",
       "      <td>2015년 청년위원회 출범 계기 청년 목소리 담은 정책 시행 도전, 희망, 행복, ...</td>\n",
       "      <td>[2015년 청년위원회 출범 계기 청년 목소리 담은 정책 시행 도전, 희망, 행복,...</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          제목  \\\n",
       "0               오거돈 \"3년간 청년정책 사업에 4,900억 투입\"   \n",
       "1                   안양청년 89명 청년정책 주도…희망발전 가동   \n",
       "2                   서울시, '2020 청년정책 협력포럼' 개최   \n",
       "3                양평군, 2020~2024년 청년정책 윤곽 드러나   \n",
       "4  대통령 표창으로 꽃 피운 대구시 청년정책…“청년이 돌아오는 대구 만들 것”   \n",
       "\n",
       "                                                  내용  \\\n",
       "0  민선 7기 2년차 첫 정책 '청년정책로드맵' 발표 2022년까지 106개 청년사업 ...   \n",
       "1  최대호 안양시장 10일 청년정책 서포터즈 위촉장 전달. 사진제공=안양시    【파이...   \n",
       "2  서울시가 '청년기본법 이후, 청년의 자리'를 주제로 '2020 청년정책 협력포럼'을...   \n",
       "3  ‘청년실태조사·정책기본계획’ 수립용역 보고회 【양평=뉴시스】 문영일 기자 = 경기 ...   \n",
       "4  2015년 청년위원회 출범 계기 청년 목소리 담은 정책 시행 도전, 희망, 행복, ...   \n",
       "\n",
       "                                              sliced embedding  cluster  세부분류  \n",
       "0  [민선 7기 2년차 첫 정책 '청년정책로드맵' 발표 2022년까지 106개 청년사업...      None       10  11.0  \n",
       "1  [최대호 안양시장 10일 청년정책 서포터즈 위촉장 전달., 사진제공=안양시    【...      None       10  11.0  \n",
       "2  [서울시가 '청년기본법 이후, 청년의 자리'를 주제로 '2020 청년정책 협력포럼'...      None       10  11.0  \n",
       "3  [‘청년실태조사·정책기본계획’ 수립용역 보고회 【양평=뉴시스】 문영일 기자 = 경기...      None       10  11.0  \n",
       "4  [2015년 청년위원회 출범 계기 청년 목소리 담은 정책 시행 도전, 희망, 행복,...      None       10  11.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('./data/youth_fewshot.json')\n",
    "df['embedding'] = None\n",
    "df = df[['제목','내용','sliced','embedding','cluster','세부분류']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model에 넣지 않고 데이터 자체에서 bert를 이용한 embedding 과정 진행\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-Medium\", do_lower_case=False)\n",
    "bert = AutoModel.from_pretrained(\"snunlp/KR-Medium\")\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "for idx in range(5) :\n",
    "    tokenized = tokenizer(df.iloc[idx,2], padding='longest', return_tensors='pt')\n",
    "    contextualized_sentences = bert(**tokenized)\n",
    "    sentence_embeddings = contextualized_sentences.pooler_output\n",
    "    df.iat[idx,3] = sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[tensor(0.2954), tensor(-0.1119), tensor(-0.4...\n",
       "1    [[tensor(0.1580), tensor(0.1035), tensor(-0.10...\n",
       "2    [[tensor(0.1997), tensor(-0.3428), tensor(-0.2...\n",
       "3    [[tensor(0.3740), tensor(-0.1149), tensor(0.00...\n",
       "4    [[tensor(0.1054), tensor(-0.1254), tensor(-0.0...\n",
       "Name: embedding, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:5,3]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = few_shot_Model(encoder).to(device)\n",
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3390222489833832\n",
      "Loss: 0.33081522583961487\n",
      "Loss: 0.3357342481613159\n",
      "Loss: 0.34250032901763916\n",
      "Loss: 0.3297666907310486\n",
      "Loss: 0.33760297298431396\n"
     ]
    }
   ],
   "source": [
    "# 같은 class 간 데이터 조합 by combinations\n",
    "from itertools import combinations, product\n",
    "for epoch in range(1) :\n",
    "    target = torch.tensor([1.0]).to(device)\n",
    "    for i,j in combinations(range(4),2) : # 안에다가 range(4) 대신 같은 class 내 데이터 인덱스 범위 넣어주면 됨 \n",
    "        f_model.train()\n",
    "        data = df[[i,j]]\n",
    "        data = tuple(d.to(device) for d in data)\n",
    "        optimizer.zero_grad()\n",
    "        output = f_model(data).unsqueeze(0)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Loss: {loss}')\n",
    "        \n",
    "# 다른 class 간 데이터 조합 by product\n",
    "    target = torch.tensor([0.0]).to(device)\n",
    "    for i,j in product(g1, g2) : # 안에다가 서로 다른 클래스들의 인덱스 집합을 넣어주면 됨\n",
    "        f_model.train() \n",
    "        data = (l1.iloc[i,3], l1.iloc[j,3])\n",
    "        data = tuple(d.to(device) for d in data)\n",
    "        target = torch.tensor(target).long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = f_model(data) \n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder만 유지하고 뒤 fc layer 등은 바꿔 끼워주며 세부분류마다 진행하면 됨\n",
    "# 각 세부분류 모델 훈련 이후 라벨링 되어있지 않은 애들은 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
