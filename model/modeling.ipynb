{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import kss\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docs</th>\n",
       "      <th>vecs</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[이르면 연말부터 전세사기 등 부동산 의심거래 AI로 잡는다, 이르면 연말쯤 전세사...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[영상] 경찰, 경기 동탄 전세사기 관련 임대인 등 압수수색, 경기 화성 동탄신도...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['전세사기 방지' 세입자 대응 강화…집주인 체납 등 정보 공개, 급격히 늘고 있는...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[올해 1분기 전국 전월세 갱신 4건 중 1건 '감액계약', 기존 계약보다 더 낮은...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"보증금 돌려달라\" 구제신청 역대 최대…전세사기 '경고등', [앵커] 최근 40대...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>[\"집주인 밀린 세금 있나\"... 임차인 '납세증명' 요구권, 법으로 보장, 체납·...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>[경찰, 화성 동탄 오피스텔 전세사기 임대인 등 압수수색, 오전 10시 30분부터 ...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>[\"전세금 못 받아 새 집 계약금 날릴 판\"... 아파트 60% '비상', 작년 2...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>[尹, 마이크 잡고 회의 주도...생방송 예정보다 1시간 넘겨, \"윤석열이라는 사람...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>[[사설] 여야, 전세사기 대책 두고 ‘네 탓 공방’ 벌일 때 아니다, 여야가 어제...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1087 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   docs vecs  label\n",
       "0     [이르면 연말부터 전세사기 등 부동산 의심거래 AI로 잡는다, 이르면 연말쯤 전세사...           0\n",
       "1     [[영상] 경찰, 경기 동탄 전세사기 관련 임대인 등 압수수색, 경기 화성 동탄신도...           0\n",
       "2     ['전세사기 방지' 세입자 대응 강화…집주인 체납 등 정보 공개, 급격히 늘고 있는...           0\n",
       "3     [올해 1분기 전국 전월세 갱신 4건 중 1건 '감액계약', 기존 계약보다 더 낮은...           1\n",
       "4     [\"보증금 돌려달라\" 구제신청 역대 최대…전세사기 '경고등', [앵커] 최근 40대...           1\n",
       "...                                                 ...  ...    ...\n",
       "1082  [\"집주인 밀린 세금 있나\"... 임차인 '납세증명' 요구권, 법으로 보장, 체납·...           0\n",
       "1083  [경찰, 화성 동탄 오피스텔 전세사기 임대인 등 압수수색, 오전 10시 30분부터 ...           0\n",
       "1084  [\"전세금 못 받아 새 집 계약금 날릴 판\"... 아파트 60% '비상', 작년 2...           1\n",
       "1085  [尹, 마이크 잡고 회의 주도...생방송 예정보다 1시간 넘겨, \"윤석열이라는 사람...           0\n",
       "1086  [[사설] 여야, 전세사기 대책 두고 ‘네 탓 공방’ 벌일 때 아니다, 여야가 어제...           1\n",
       "\n",
       "[1087 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('./data/전세사기_라벨링.json')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet & Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,0].tolist(), df.iloc[:,1].tolist(), test_size=0.2, random_state=7, stratify=df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=7, stratify=y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self, data, label):\n",
    "    super().__init__()\n",
    "    self.docs = data\n",
    "    self.label = label\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.docs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    news = self.docs[index]\n",
    "    target = self.label[index]\n",
    "    return news, target\n",
    "\n",
    "train_data = MyDataset(data=X_train, label=y_train)\n",
    "val_data = MyDataset(data=X_val, label=y_val)\n",
    "test_data = MyDataset(data=X_test, label=y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "  def __init__(self, file_path):\n",
    "    super().__init__()\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-Medium\", do_lower_case=False)\n",
    "    self.bert = AutoModel.from_pretrained(\"snunlp/KR-Medium\")\n",
    "    for param in self.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    df = pd.read_json(file_path)\n",
    "    for idx in range(len(df)) :\n",
    "      tokenized = self.tokenizer(df.iloc[idx,0], padding='longest', return_tensors='pt')\n",
    "      contextualized_sentences = self.bert(**tokenized)\n",
    "      sentence_embeddings = contextualized_sentences.pooler_output\n",
    "      df.iat[idx,1] = sentence_embeddings\n",
    "    self.vecs = df['vecs']\n",
    "    self.label = df['label']\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.vecs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    docs = self.vecs[index]\n",
    "    label = self.label[index]\n",
    "    return docs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset('data/전세사기_라벨링.json')\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# sentence_embeddings = j[0]\n",
    "# max_len = max([e.size(0) for e in sentence_embeddings])\n",
    "# padded_embeddings = torch.zeros(len(sentence_embeddings), max_len, sentence_embeddings[0].size(1))\n",
    "# for i, emb in enumerate(sentence_embeddings):\n",
    "#     seq_len = emb.size(0)\n",
    "#     padded_embeddings[i, :seq_len, :] = emb\n",
    "    \n",
    "# random_tensor = torch.randn(padded_embeddings.size(0), 1, padded_embeddings.size(2))\n",
    "# batch_tensor = torch.cat((random_tensor, padded_embeddings), dim=1)\n",
    "# batch_t = batch_tensor.permute(1 ,0 ,2).float()\n",
    "\n",
    "# padding_mask = batch_t.sum(dim=-1).permute(1 ,0) == 0\n",
    "# title_level = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=768, nhead=1),num_layers=1)\n",
    "# output_batch = title_level(batch_t.float(), src_key_padding_mask=padding_mask)\n",
    "# output_batch = output_batch.permute(1 ,0 ,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Structure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for문 Ver.\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, num_classes, input_dim, num_heads, num_layers):\n",
    "    super().__init__()\n",
    "    self.fc = nn.Linear(input_dim, num_classes)\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-Medium\", do_lower_case=False)\n",
    "    self.bert = AutoModel.from_pretrained(\"snunlp/KR-Medium\")\n",
    "    for param in self.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    self.title_level = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads),\n",
    "      num_layers=num_layers)\n",
    "    self.sentecne_level = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads),\n",
    "      num_layers=num_layers)\n",
    "\n",
    "  def forward(self, data):\n",
    "    doc_ems = torch.empty((len(data), 768)).to(device)\n",
    "    for num, doc in enumerate(data):\n",
    "      tokenized = self.tokenizer(doc, padding='longest', return_tensors='pt').to(device)\n",
    "      with torch.no_grad():\n",
    "        contextualized_sentences = self.bert(**tokenized)\n",
    "      sentence_embeddings = contextualized_sentences.pooler_output\n",
    "      title = sentence_embeddings[0, :]\n",
    "      transformer_output = self.title_level(sentence_embeddings)[1:, :]\n",
    "      combined_input = torch.cat((title.unsqueeze(1).view(1, 768), transformer_output), dim=0)\n",
    "      doc_ems[num] = self.sentecne_level(combined_input)[0]\n",
    "    out = self.fc(doc_ems)\n",
    "    predicted_label = torch.argmax(out).to(device)\n",
    "    return out, predicted_label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Batch Ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc_Encoder(nn.Module):\n",
    "    def __init__(self, num_classes, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.sentecne_level = nn.TransformerEncoder(\n",
    "          nn.TransformerEncoderLayer(d_model=768, nhead=num_heads),\n",
    "          num_layers=num_layers)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        max_len = max([e.size(0) for e in batch])\n",
    "        padded_embeddings = torch.zeros(len(batch), max_len, batch[0].size(1)).to(device)\n",
    "        for i, emb in enumerate(batch):\n",
    "            seq_len = emb.size(0)\n",
    "            padded_embeddings[i, :seq_len, :] = emb\n",
    "        random_tensor = torch.randn(padded_embeddings.size(0), 1, padded_embeddings.size(2)).to(device)\n",
    "        batch_tensor = torch.cat((random_tensor, padded_embeddings), dim=1)\n",
    "        batch_tensor = batch_tensor.permute(1 ,0 ,2).float()\n",
    "        padding_mask = batch_tensor.sum(dim=-1).permute(1 ,0) == 0\n",
    "        output_batch = self.sentecne_level(batch_tensor.float(), src_key_padding_mask=padding_mask)\n",
    "        output_batch = output_batch.permute(1 ,0 ,2)\n",
    "        doc_vecs = output_batch[:,0,:]\n",
    "        return doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module) :\n",
    "    def __init__(self, num_classes, num_heads, num_layers) :\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.encoder = Doc_Encoder(num_classes, num_heads, num_layers)\n",
    "    \n",
    "    def forward(self, batch) :\n",
    "        doc_vecs = self.encoder(batch)\n",
    "        out = self.fc(doc_vecs)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter & Schedular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "num_classes = 2\n",
    "input_dim = 768\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "model = Model(num_classes, num_heads, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_data, target in dataloader:\n",
    "        batch_data, target = batch_data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: 0.7501271367073059\n",
      "Batch 2, Loss: 1.423123836517334\n",
      "Batch 3, Loss: 6.371459007263184\n",
      "Batch 4, Loss: 2.942178726196289\n",
      "Batch 5, Loss: 2.4286482334136963\n",
      "Batch 6, Loss: 2.924318790435791\n",
      "Batch 7, Loss: 1.4806333780288696\n",
      "Batch 8, Loss: 0.730457067489624\n",
      "Batch 9, Loss: 1.3733482360839844\n",
      "Batch 10, Loss: 0.5560861229896545\n",
      "Batch 11, Loss: 0.49686524271965027\n",
      "Batch 12, Loss: 0.6685193181037903\n",
      "Batch 13, Loss: 0.7135567665100098\n",
      "Batch 14, Loss: 0.7412486672401428\n",
      "Batch 15, Loss: 0.764586329460144\n",
      "Batch 16, Loss: 0.6955113410949707\n",
      "Batch 17, Loss: 0.6259647607803345\n",
      "Batch 18, Loss: 0.6809948682785034\n",
      "Batch 19, Loss: 0.8341611623764038\n",
      "Batch 20, Loss: 0.6566769480705261\n",
      "Batch 21, Loss: 0.6850721836090088\n",
      "Batch 22, Loss: 0.6530672907829285\n",
      "Batch 23, Loss: 0.6493446826934814\n",
      "Batch 24, Loss: 0.7069054245948792\n",
      "Batch 25, Loss: 0.6934828162193298\n",
      "Batch 26, Loss: 0.718299150466919\n",
      "Batch 27, Loss: 0.6881782412528992\n",
      "Batch 28, Loss: 0.7267142534255981\n",
      "Batch 29, Loss: 0.7038354277610779\n",
      "Batch 30, Loss: 0.670619785785675\n",
      "Batch 31, Loss: 0.5711156725883484\n",
      "Batch 32, Loss: 0.7834558486938477\n",
      "Batch 33, Loss: 0.7485091686248779\n"
     ]
    }
   ],
   "source": [
    "model.train()  # 모델을 훈련 모드로 설정\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    data = tuple(d.to(device) for d in data)\n",
    "    target = torch.tensor(target).long().to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data) \n",
    "    loss = criterion(output, target)\n",
    "    print(f'Batch {batch_idx+1}, Loss: {loss.item()}')\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = model.to(device)\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data, target in train_loader:\n",
    "        target = torch.tensor(target).long()\n",
    "        target = target.to(device)\n",
    "\n",
    "        out = model(batch_data)\n",
    "        predicted = torch.argmax(out, dim=1)\n",
    "        total_samples += target.size(0)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6117424242424242\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few-shot learning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class few_show_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.encoder = Doc_Encoder(768,1,1)\n",
    "        self.fc_layer = nn.Linear(768,768)\n",
    "        self.similarity = F.cosine_similarity()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, doc_pair):\n",
    "        doc_vecs = self.encoder(doc_pair)\n",
    "        doc_vecs = self.fc_layer(doc_pair)\n",
    "        similarity = self.similarity(doc_vecs)\n",
    "        out = self.sigmoid(similarity)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 class 내 2개씩 pair & 서로 다른 class 1개씩 pair해서 모두 훈련"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
