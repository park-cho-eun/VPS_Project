# -*- coding: utf-8 -*-
"""modeling.ipynb의 사본의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_nuIbVO9qFV33LTQrlcYNks_MsdauGym
"""

!pip install transformers

#훈련용, 테스트용 분할
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(data3['news'].tolist(), data3['aware'].tolist(), test_size=0.2, random_state=42, stratify=data3['aware'])

"""모델링 시작"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from torch.optim import Adam

device = torch.device("cuda")

import tqdm
import torch
from torch.utils.data import Dataset, DataLoader

#데이터셋 클래스 재정의
class MyDataset(Dataset):
  def __init__(self, data, label):
    super().__init__()
    self.docs = data
    self.label = label

  def __len__(self):
      return len(self.docs)

  def __getitem__(self, index):
    news = self.docs[index]
    target = self.label[index]
    return news, target

#데이터셋, 데이터로더 객체 생성
trainset = MyDataset(data=X_train, label=y_train)
test_data = MyDataset(data=X_test, label=y_test)

batch_size = 32
train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=True, collate_fn=lambda x: tuple(zip(*x)))

#모델 재정의
class Model(nn.Module):
  def __init__(self, num_classes, input_dim, num_heads, num_layers):
    super().__init__()
    self.fc1 = nn.Linear(input_dim, 384)
    self.relu = nn.ReLU()
    self.fc2 = nn.Linear(384, num_classes)

    self.tokenizer = AutoTokenizer.from_pretrained("snunlp/KR-Medium", do_lower_case=False)
    self.bert = AutoModel.from_pretrained("snunlp/KR-Medium")
    self.transformer_encoder = nn.TransformerEncoder(
      nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads),
      num_layers=num_layers)

  def forward(self, data):
    doc_ems = torch.empty((len(data), 768)).to(device)
    for num, doc in enumerate(data):
      tokenized = self.tokenizer(doc, padding=True, truncation=True, return_tensors='pt', max_length=512, is_split_into_words=True).to(device)
      with torch.no_grad():
        contextualized_sentences = self.bert(**tokenized)
      sentence_embeddings = contextualized_sentences.pooler_output.to(device)
      title = sentence_embeddings[0, :]

      # Transformer Encoder을 적용해 title sentence의 semantic을 각 문장들에 반영
      transformer_output = self.transformer_encoder(sentence_embeddings)[1:, :]

      # title과 title의 semantic을 반영한 각 문장들의 embedding vectors를 Transformer Encoder를 통과시켜 doc embedding 추출
      combined_input = torch.cat((title.unsqueeze(1).view(1, 768), transformer_output), dim=0).to(device)
      doc_embedding = self.transformer_encoder(combined_input).to(device)

      # 평균 풀링을 통해 문서 레벨의 임베딩을 추출
      doc_embedding = torch.mean(doc_embedding, dim=0)
      doc_ems[num] = doc_embedding

    # FC-layer & Softmax
    out = self.fc1(doc_ems)
    out = self.relu(out)
    out = self.fc2(out)
    out = nn.functional.softmax(out).to(device)  # nn.Softmax()를 nn.functional.softmax()로 변경
    predicted_label = torch.argmax(out).to(device)

    return out, predicted_label

#모델, 손실함수, 활성화함수 객체 생성
model = Model(num_classes=2, input_dim=768, num_heads=1, num_layers=2).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = Adam(model.parameters(), lr=1e-3) #1e-5는 학습이 너무 느려서 1e-3으로 수정

#모델 학습 수행
num_epochs = 10
model.train()

for epoch in tqdm.tqdm(range(num_epochs)):
  total_loss = 0.0
  for batch_data, target in train_loader:
    target = torch.Tensor(target).long()
    target = target.to(device)

    optimizer.zero_grad()

    outputs, _ = model(batch_data)

    loss = criterion(outputs, target)
    loss.backward()

    optimizer.step()

    total_loss += loss.item()

  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}")
print("Training complete!")

#학습 파라미터 및 학습 모델 저장
torch.save({"model": "CustomModel",
            "epoch": 50,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "cost": total_loss,
            "description": "CustomModel epoch-50"},
           "/content/drive/MyDrive/vps/2layers-50epochs.pt")

#학습한 결과 중 가장 정확도가 높은 학습 모델 확인
import io

for num in range(1, 136):
  model = Model(num_classes=2, input_dim=768, num_heads=1, num_layers=2).to(device)
  checkpoint = torch.load(f'/content/drive/MyDrive/vps/2layers-{num}.pt')
  model.load_state_dict(checkpoint["model_state_dict"])

  total_correct = 0
  total_samples = 0

  with torch.no_grad():
    for batch_data, target in test_loader:
        target = torch.Tensor(target).long()
        target = target.to(device)

        outputs, _ = model(batch_data)
        _, predicted = torch.max(outputs, 1)

        total_samples += target.size(0)
        total_correct += (predicted == target).sum().item()

  accuracy = total_correct / total_samples
  print(f"Test Accuracy for {num}-model: {accuracy * 100:.2f}%")